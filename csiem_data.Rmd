# CSIEM : data

## Background and Context

CSIEM seeks to integrate a diverse range of data from various origins and providers in order to allow the Cockburn Sound models developed to be informed and integrated with the best available data and science. However, the many types of data, such as weather, hydrologic, water quality, or ecological data, and the fact that the data spans many decades, creates a challenge to effectively organise the entire data-set in a robust, repeatable and transparent way. Several of the key data-sets are managed by different agencies who each use their own technical frameworks and standards for data organisation, and many other data-sets remain in an unorganised form, so intgerating all the diverse datasets requires development of a common set of metadata standards and reliable data processing workflows. Providing this coordination to bring data-sets together is referred to as [data federation](https://www.sciencedirect.com/topics/computer-science/data-federation).

Data that is relevant for the management and modelling of Cockburn Sound comes from national data collection programs, state agency environmental monitroing programs, local companies and organisations, various initiatives within the research community. These datasets are _all_ critical for understanding the Cockburn Sound ecosystem and form the foundation of the CSIEM system. Beyond simply organising data, the needs of the modelling requires data in a consistent format (e.g. units, terminology, data structure, etc), to allow for effective model-data integration as is required during the model set up, validation and reporting phases. With this regard, workflows have been developed to organise and store both field observational data and model data in a consistent way. The details of CSIEM data organisation are described in this chapter (Chapter 3), and the methodology for model-data integration is described in [Chapter 4](Model-Data Organisation).

This Chapter summaries a synthesis of the various data sources into a standard, _CSIEM Environmental Information Management_ framework that sets out standards and workflows. This framework is needed to :

- Support a _federation_ of datasets, and
- Document the standards used to organise observational and modelled data, using a flexible metadata and workflow setup

This will provide:

- Standards for data users,
- Compiled data products for ease of use,
- An entry point for new data generators who want to add new data products, and
- A way to facilitate model and data integration for CSIEM model assessment, visualisation and analytics.

<br>

## Relevant data management initiatives

Various information management activities are currently active and relevant to Cockburn Sound Environmental Management. These include:

- Microsoft Azure Blob Storage
- WAMSI GIS Portal
- WAMSI Sharepoint
- Pawsey Acacia Storage
- BMT Sharepoint
- DWER Water Information Reporting system
- DOT Coastal Data system
- Other 3rd party data facilities (e.g. BOM, IMOS etc)

<br>

An overview and relevant links of the key related initiatives are listed below.

<br>

::: {.panelset .sideways}

::: panel
[Azure Blob]{.panel-name}
**Microsoft Azure Blob Storage**

The Azure Blob Storage was set up by Westport as a location for _WAMSI Westport Marine Science Program_ data to be stored, and in some cases, historical data. This storage option is suitable for medium to large sized data (1 - 100GB) e.g., raster bathymetry data. Access to the Azure Blob Storage is available to Westport staff and selected WAMSI Theme 10 data team members. Data and reports will become accessible for viewing and downloading only to all involved in the WAMSI Westport Marine Science Program and Westport Office through links with the WAMSI GIS portal.

```{r data-pic1, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Microsoft Azure Blob storage."}

knitr::include_graphics("images/cdm/blob.png")

```

:::

::: panel
[WAMSI GIS]{.panel-name}
**WAMSI GIS Portal**

The WAMSI GIS Portal is a Westport ArcGIS managed website that acts as a navigation centre for researchers accessing commonly requested datasets and documents, both historical and provided under the WAMSI Westport Marine Science Program. The home page is intended to provide summary information and search functionality and will be the default landing page when researchers log into their Westport ArcGIS accounts. Information such as the most up to date Cockburn Sound data audit, Theme 10 workshop slides and Westport Technical Standards can be found here. Contrary to appearance, the WAMSI Portal does not ‘host’ data - rather, it provides searchable listings that link to data and reports stored on one of the four data stores: Westport SDE (Spatial Database Engine), Azure Blob Storage, WAMSI Sharepoint (mentioned above), Pawsey Acacia Storage.

```{r data-pic2, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "WAMSI GIS portal."}

knitr::include_graphics("images/cdm/GIS_Portal.png")

```

:::

::: panel
[WAMSI Sharepoint]{.panel-name}
**WAMSI Westport Reports and Data Sharepoint**

WAMSI set up a Microsoft Sharepoint page to store historical Cockburn Sound data and reports acquired from proponents. Some of the data and reports and copied across from the BMT Sharepoint page, and others are obtained outside of BMT. Small datasets collected under the current WAMSI Westport Marine Science Program may also be stored here. The data and reports are sorted under a folder structure with the first folder division being ‘Open access use’ or ‘WAMSI Westport use’. Data and reports are then organised by proponent/organisation -> year -> report title. Document and folder URLs from the WAMSI Sharepoint will be linked to the WAMSI GIS portal to become searchable by WAMSI Westport Marine Science Program researchers and the Westport Office. The WAMSI Sharepoint is currently accessible to WAMSI staff and selected Theme 10 data team members, and will become accessible (view and download only) to all involved in the WAMSI Westport Marine Science Program and Westport Office through links with the WAMSI GIS portal.

```{r data-pic1a, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Microsoft Sharepoint storage."}

knitr::include_graphics("images/cdm/blob.png")

```

:::

::: panel
[Acacia Storage]{.panel-name}
**Pawsey Acacia Storage**

The Pawsey Acacia Storage is intended for the largest of datasets (100s-1000s GB) sourced from external proponents or the WAMSI Westport Marine Science Program e.g. Theme 5's ROMs oceanography model output. Access to the [Pawsey Acacia Storage](https://pawsey.org.au/systems/acacia/) is available to Westport staff and selected WAMSI Theme 10 data team members. Data is restricted for viewing and downloading to those involved in the WAMSI Westport Marine Science Program and Westport Office through links with the WAMSI GIS portal.

```{r data-pic3, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Pawsey Acacia storage."}

knitr::include_graphics("images/cdm/Acacia.png")

```

:::

::: panel
[BMT Sharepoint ]{.panel-name}
**BMT Cockburn Sound Data SharePoint**

BMT Commercial Australia Pty Ltd (BMT, previously Oceanica) is a large and established consultancy that monitors and collects marine and coastal data within Cockburn Sound for a variety of proponents, particularly Fremantle Ports, Water Corporation and Cockburn Cement Limited. BMT was engaged by WAMSI as they hold proponent’s data and are best placed to share data with WAMSI once sharing permissions are received from proponents. BMT set up a Microsoft Sharepoint page that contains a catalogue of reports and data held by BMT, data sharing license agreements from proponents, and requested reports and data related to the data sharing license agreements. WAMSI then migrates the approved reports and data across to a WAMSI Sharepoint. Access to the BMT Sharepoint has been provided to selected Theme 10 data team members and BMT staff. BMT also set up restricted access folders within the BMT Sharepoint for major proponents to easily review their reports prior to giving access to WAMSI. WAMSI Theme 10 data team members can not access these restricted proponent folders.

:::

::: panel
[DWER WIR ]{.panel-name}
**DWER Water Information Reporting & Cockburn Mooring Data**

[Water Information Reporting](https://estuaries.dwer.wa.gov.au/cockburn-sound/)

<br>

[Mooring Data](https://estuaries.dwer.wa.gov.au/cockburn-sound/)

...
:::

:::

<br>

## CSIEM environmental data management

The goal of the CSIEM Environmental Information Management framework, as presented herein, is to allow compatibility, inter-operability and between crticial data assets, and version control as is required for the development of a comprehensive and integrated modelling platform.

The framework can be viewed as three separate systems:

- Data Collation
- Data Governance & Reporting
- Data Integration

The relationship between the various iniatives, the CSIEM Environmental Information Management framework, and downstream model applications are outlined schematically in Figure \@ref(fig:data-pic4).

<br>

```{r data-pic4, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "CSIEM Environmental Information Management overview (click to enlarge). "}

knitr::include_graphics("images/cdm/Information_Management.png")

```

### Data Collation

The aim of the data collation step is to bring data together in a co-ordinated way. Data that is sourced and collated from various government agencies, researchers and industry groups is stored in a "data lake" (see Section 3.3.3) in their raw format. Each data provider is assigned a unique agency identifier, and datasets are also grouped based on the main programs or iniatives the collection was associated with. Raw data is stored in a rigid folder structure based on these two identifiers : <br> <br> `Agency/Program/ < ... data-sets ...> `

```{r data-pic5, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Data flow into the date lake. Raw data is allocated to folders based on the agency and program of orgin, and version controlled within the csiem-data GitHub repository (right box)."}

knitr::include_graphics("images/cdm/data_collation.png")

```

Data stored within the data lake is not formatted or processed in any way. Removal of duplicated and depreciated datasets is handled by the ETL (**"Extract, Transform, Load"**) pipelines outlined in Section 3.3.3. As such, strict data governance is required to maintain control and understanding of the contents of the data lake.

### Data Governance

Data Governance is the process of managing the availablity and usablity of data within the _CSIEM Environmental Information Management_ system. Governance can be loosely broken up into four key areas:

- Classification
- Cataloguing
- Mapping
- Versioning

<!--
```{r data-pic6, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Data Governance."}

knitr::include_graphics("images/cdm/governance.png")

```
-->

Each step has been designed to allow the users of the system to quickly understand the avalaiable data contained within the lake, the spatio-temporal extent that each data-set covers, as well as identify areas of missing or unavailable data (Figure \@ref(fig:data-pic7)). The final step is a version controlled repository of the data, stored within the data-lake folder of the csiem-data GitHub repository.

```{r data-pic7, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Steps for governance of new and updated data-sets."}

knitr::include_graphics("images/cdm/Governance_Pipeline.png")

```

#### Data-set classification

Each data-set that is added to the data lake is classified with a standard set of attributes (see Section 3.3.2.2), the three most important being:

- Agency
- Program
- Data Type

In order to maintain the usability of the data lake, given it's unstructured nature, it's important to ensure consistancy when deciding where a particular data-set is to be stored. In the domain of the CSIEM Environmental Information Management system, all raw data is stored in the strict folder stucture `agencycode/programcode`. If a particular agency program is unknown, a program code must be assigned during the cataloguing proceedure. Multiple data-sets collected as part of a program can be organised freely.

Each dataset is also given a specific data category to facilitate user searches and downstream conversions and integration with the modelling tasks (see CHapter 4). Data is categorised in the general areas of :

- _Bathymetry_
- _Meteorology_
- _Hydrology_
- _Water Quality_
- _Sediment_
- _Biota_
- _Knowledge_
- _Operational_

<br>

These are defined into sub-categories, as outlined in Table \@ref(tab:data-classes).

```{r data-classes, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/CDM_Classes.xlsx', sheet = 2)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,3:4], caption = "CSIEM data categories used for classification of incoming data-sets.", align = "l",) %>%

row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

<br>

#### Cataloguing

All data entering the sytem is recording in the `csiem_data_catalogue.xlsx` spreadsheet that can be found the `data-lake` directory in the `cseim-data` GitHub repository. A full outline of the latest CSIEM data catalogue is included in **Appendix A: Data Catalogue**. A summary overview diagram is shown below in \@ref(fig:data-flow) .

```{r data-flow, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Data catalogue overview showing data sources organised by agency and programs (under development; click to enlarge)."}

knitr::include_graphics("images/cdm/dataflow.png")

```

<br>

#### Station/Site mapping

Independant GIS files are produced for each dataset added to the data lake. Not only does this allow for ease of mapping and visualisation, it serves as an important cross check of the ingestion pipelines and any co-ordinate system conversions that have occurred within the pipeline.

Each dataset has a corresponding `csv` file saved within the `data-mapping` directory of the `csiem-data`, with the file name corresponding the the agencycode/programcode outlined above.

<br>

```{r data-mapping, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/DOT_TIDE.xlsx', sheet = 1)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,2:10], caption = "Example data-set GIS station location file, standarised for data-mapping.", align = "l",) %>%

row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

<br>

<br>

### Data Integration

The _CSIEM Environmental Information Management_ approach utilises a **Data Lake** --> **Data Warehouse** data management system, driven via custom batch **"ETL"** functions, triggered via GitHub actions or able to be run manually. This system has been implimented due to both the vast array of raw data inputs required for integarted modelling, as well as the diverse data analyical demands required across different agences and platforms.

#### GitHub

GitHub is a cloud based git repository service, with enterprise level version control and tracking. It has been chosen as the central repository for the data and the pipeline functions based upon it's maturity as a platform, as well as it's widespread adoption within industry and academia.

The `csiem-data` repository can be viewed and cloned [here](https://github.com/SEAF-CS/csiem-data)

#### Data Lake

A **Data Lake** is simply a centralised store of raw, disparate datasets, be it structured or unstructured. Centralising and cataloguing this raw data allows for customised **ETL** processes to be constructed that can be tailored to each analytics usecase, as opposed to forcing the data into a **one size fits all** structure.

However, because the data is primarily being stored in it's raw format, goverance beomes extremely important so that data assests are not lost in the lake.

#### Data Warehouse

The **Data Warehouse** is the store of boutique, costomised data products produced through the batch **ETL (Extract, Transform and Load)** pipelines. Each folder within the `data-warehouse` directory on GitHub contains processed data in different formats, based upon end user requirements.

All data usage outside of the ETL pipelines should and must be carried out from products within the warehouse. This will ensure both the efficient and repeatability of any script or data product produced downstream, as well as providing a constant data pathway for data validation.

#### Data Pipelines & ETL

Each dataset found within the data-lake is extracted via the **ETL** pipeline ustilising code found in the `code\import` directory. As each dataset is stored in it's raw form, extraction and transformation is more often than not going to require a unique codebase. As such, a unified pipeline is not being developed at this stage, and each data source will have it's own codebase. To facilitate this, each code base will draw upon share glossaries (such as the variable glossay discussed below) and conversion tables, to ensure standardisation.

```{r data-pic9, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "ETL (Extract, Transform, Load) data workflow diagram."}

knitr::include_graphics("images/cdm/ETL.png")

```

#### GitHub Actions

One of the primary reason for choosing GitHub to house the CSIEM Environmental Information Management system is GitHub Actions. GitHub actions allows for the automation of workflows, using a variety of languages and technologies. Pipelines constructed in both Pyhton and Matlab can be automated and run in parrallel via simple configurations. Pipelines can be constructed that can be automated and processed on a wide variety of platforms and operating systems.

#### Variable Standardisation

Fundamental to the data collation program is the concept of data standardisation. All data contained within the data lake is processed nightly into standardised data products.

As part of this standardisation, all incoming variables need to be converted into a common variable name and unit. To facilitate this, a global variable translation glossary is being developed, which will allow the data engineers to quickly and transparently create importation scripts to process new datasets.

Master variables are first defined, with a unique identifier, variable name, units and symbol (if applicable) Table \@ref(tab:variable-classes).

<br>

```{r variable-classes, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/variable_key_extract.xlsx', sheet = 1)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,2:9], caption = "CSIEM variable classification,", align = "l",) %>%

row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

<br>

Once a new data source has been added to the data lake, a custom key is defined for the new variables and unit conversions created to standardise the dataset to the ingestion data standard (see Chapter 4) \@ref(tab:dwer-classes).

<br>

```{r dwer-classes, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/variable_key_extract.xlsx', sheet = 2)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,2:5], caption = "DWER agency key used for variable mapping and conversion,", align = "l",) %>%

row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

<br>

### Model-derived Data Governance

Model derived data is used extensively throughout the research program, from providing direct input into other models (nesting) through to model validation. As such, model governance plays an equally import role as data governance. It is treated separately as model data is not stored directly in a single lake environment, but accessed and processed in a variety of ways depending on it's size and storage location.

The Pawsey Acacia Storage is the primary home for model data produced by the different project teams. This storage space represents the long-term point of truth storage for these products. However, thrid-party produced models, as well as curated secondarily produced products derived from primary models will also be stored temporarily on Acicia for the duration of the project. Model data such as BOM's BARRA-R output that is used directly as an input into a model, or used as a vailable source for other data will be downloaded and stored, with any workflow's produced directly referencing that Pawsey stored data.

```{r data-model, echo = FALSE, out.width='100%', class = "text-image", fig.cap = "Model governance and storage."}

knitr::include_graphics("images/cdm/model_product.png")

```

<!---
<br>

```{r model-classes, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/CSIEM_Model_Catalogue_Extract.xlsx', sheet = 1)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,3:13], caption = "CSIEM model catalogue (currently under development). ", align = "l",) %>%
  pack_rows(theSheetGroups[1],
            min(which(theSheet$Group == theSheetGroups[1])),
            max(which(theSheet$Group == theSheetGroups[1])),
            background = '#ebebeb') %>%
  pack_rows(theSheetGroups[2],
            min(which(theSheet$Group == theSheetGroups[2])),
            max(which(theSheet$Group == theSheetGroups[2])),
            background = '#ebebeb') %>%
  pack_rows(theSheetGroups[3],
					  min(which(theSheet$Group == theSheetGroups[3])),
					  max(which(theSheet$Group == theSheetGroups[3])),
					  background = '#ebebeb') %>%
  pack_rows(theSheetGroups[4],
          	min(which(theSheet$Group == theSheetGroups[4])),
          	max(which(theSheet$Group == theSheetGroups[4])),
          	background = '#ebebeb') %>%
  pack_rows(theSheetGroups[5],
          	min(which(theSheet$Group == theSheetGroups[5])),
          	max(which(theSheet$Group == theSheetGroups[5])),
          	background = '#ebebeb') %>%
row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)

```
```
--->

<br>

## Adding New Data Sources

The data repository is a constantly evolving collection that can be automatically updated for existing data sources, or new data sources can be added at any time. In order to maintain the constancy and quality of the data products stored in the data warehouse, it's important to follow the below outlines steps when adding new data sources. These steps are:

- Add raw data to the data-lake directory, filing it under agency and project subdirectories.
- Update the CSIEM_Data_Catalogue.xlsx found in the data-governance directory
- Check & update the variable_key.xlsx spreadsheet with any new variables
- Add a new conversion tab in the variable_key.xlsx spreadsheet for the variable conversions required for the new data
- Add new sites to the site_key.xlsx
- Create a new import ETL script in the code/import directory to import the new data into the warehouse base format (csv).
- If the data extraction is to be updated daily, add new lines into the code/actions scripts to trigger your import function.
- Add a new site to the obs_maps.csv to the csiem-science/maps/csiem_data/Obs directory in the csiem-science GitHub repository.




## Data repository and management

The details of data collection, processing, and catagorizing have been described in Chapter 3.3 and Appendix A. This section introduces the data repository and management related to the data-model integration framework.

### **Data repository folder structure**

The data repository folder structure follows the logic of data types and resources as shown in Figure \@ref(fig:csiem-pic7).

```{r csiem-pic7, echo = FALSE, out.width='100%', class = "text-image",fig.align='center', fig.cap = "CSIEM Data online storage folder structure"}

knitr::include_graphics("images/cdm/csiem_data_folder_structure.png")

```

The root directories are listed and described below:

```{r A-folders, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/folders.xlsx', sheet = 1)
theSheet <- theSheet[theSheet$Table == "data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,2:3], caption = "CSIEM Root Folder Descriptions", align = "l",) %>%

row_spec(0, background = "#14759e", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

### **Data catalogue**

All data that is uploaded to the CSIEM github repository is logged in the `csiem-data` CSIEM Data Catalogue. The catalogue (found in the data-governance directory) is comprised of a main data sheet ("CSIEM Data Catalogue") as well as summary sheets for all processed data. Every raw datafile is logged with the following information:

```{r A-metadata, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(readxl)
library(rmarkdown)
theSheet <- read_excel('tables/CSIEM_Classes.xlsx', sheet = 1)
theSheet <- theSheet[theSheet$Table == "Data",]
theSheetGroups <- unique(theSheet$Group)

kbl(theSheet[,3:4], caption = "CSIEM Catalogue Metadata", align = "l",) %>%

row_spec(0, background = "#002B4D", bold = TRUE, color = "white") %>%
  kable_styling(full_width = T,font_size = 10) %>%
    scroll_box(fixed_thead = FALSE)
```

<br>

A catalogue of all relevant data under management is provided in [Appendix A](Appendix A : Data Catalogue).

<br>
<!--
Once imported, the data is then summerised via a script in the ```scripts/dataimport/summary``` directory to create a standard set of plots and summary tables that can be found in the ```data/summary``` directory. ESRI shapefiles are also automatically produced and saved into the ```gis/summary``` directory to be used to create a stardarised site map of the data's sampling locations.

```{r csiem-pic10, echo = FALSE, out.width='100%', class = "text-image",fig.align='center', fig.cap = "Example GIS Map showing the sampling locations for the Water Data SA online data "}

knitr::include_graphics("images/cdm/DEW WaterDataSA Locations.png")
```

-->

### **ETL (Extract, Transform and Load) workflows**

All of the ETL pipeline scripts for the CSIEM Environmental Information Management are all contained within the `code/` directory, which has three subfolders:

- `import`: All scripts that process the raw data from the data-lake into the data-warehouse.
- `actions`: GitHub action functions, and well as secondary functions to control with pipeline is executed automatically.
- `functions`: Shared function that scripts within the above directories may call.

Scripting has been predominately carried out utilising the [Matlab](https://au.mathworks.com/?s_tid=gn_logo) programming language, however there are scripts within the `scripts` directory using both [R](https://www.r-project.org/about.html) and [Python](https://www.python.org/). Users are able to add scripts in their preferred language; the repository core scripts for data storage and model plotting are currently in Matlab.

Below is an example workflow outlining how a meteorological boundary condition file for the TUFLOW-FV model is created (Figure \@ref(fig:csiem-pic9). Data is first downloaded and processed by scripts in the `code` directory. Standardised data is then stored in the `data-warehouse` for usage in the creation of model boundary input file and in the MARVL visualisation and analytics package.

```{r csiem-pic9, echo = FALSE, out.width='100%', class = "text-image",fig.align='center', fig.cap = "CSIEM work flow diagram for met data"}

knitr::include_graphics("images/cdm/Met_Workflow.png")

```

<br>


